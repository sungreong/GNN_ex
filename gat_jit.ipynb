{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "dataset = 'Cora'\n",
    "dataset = Planetoid(\"./data\", dataset, transform=T.NormalizeFeatures())\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GATConv(dataset.num_features, 8, heads=8,\n",
    "                             dropout=0.6).jittable()\n",
    "\n",
    "        self.conv2 = GATConv(64, dataset.num_classes, heads=1, concat=True,\n",
    "                             dropout=0.6).jittable()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = Net().to(device), dataset[0].to(device)\n",
    "model = torch.jit.script(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data.x, data.edge_index), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "import sys\n",
    "class Printer():\n",
    "    \"\"\"Print things to stdout on one line dynamically\"\"\"\n",
    "    def __init__(self,num_period=10):\n",
    "        self.num_period = num_period\n",
    "        self.init_value = 0\n",
    "\n",
    "    def __call__(self,data) :\n",
    "        if self.init_value % self.num_period == 0 :\n",
    "            print('\\n'+data.__str__())\n",
    "            self.init_value = 1\n",
    "        else :\n",
    "            sys.stdout.write(\"\\r\\x1b[K\"+data.__str__())\n",
    "            sys.stdout.flush()\n",
    "            self.init_value += 1 \n",
    "            \n",
    "printf = Printer(num_period=50)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    printf(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch: 001, Loss: 1.9440, Train: 0.4786, Val: 0.2560, Test: 0.2870\n",
      "\u001b[KEpoch: 050, Loss: 1.4688, Train: 0.9286, Val: 0.7640, Test: 0.7690\n",
      "Epoch: 051, Loss: 1.4181, Train: 0.9286, Val: 0.7620, Test: 0.7650\n",
      "\u001b[KEpoch: 100, Loss: 0.9614, Train: 0.9857, Val: 0.8180, Test: 0.8270\n",
      "Epoch: 101, Loss: 0.9251, Train: 0.9857, Val: 0.8200, Test: 0.8220\n",
      "\u001b[KEpoch: 150, Loss: 0.7430, Train: 0.9857, Val: 0.8160, Test: 0.8240\n",
      "Epoch: 151, Loss: 0.7614, Train: 0.9857, Val: 0.8180, Test: 0.8250\n",
      "\u001b[KEpoch: 200, Loss: 0.6708, Train: 0.9929, Val: 0.8020, Test: 0.8230"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}