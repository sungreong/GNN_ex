{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 이전 step까지 노드 분류 작업을 위해 전체 배치 방식으로만 그래프 신경망을 훈련했습니다. \n",
    "    * 특히, 이는 모든 노드의 은닉 표현이 병렬로 계산되었고 다음 계층에서 재사용할 수 있음을 의미합니다.\n",
    "    * 그러나 큰 그래프에서 작업을 수행하고자 하면 메모리 소비가 폭발적으로 증가하기 때문에 이 계획은 더 이상 실현 가능하지 않습니다. \n",
    "    * 예를 들어 약 천만 개의 노드와 128개의 hidden feature 차원이 있는 그래프는 이미 각 계층에 대해 약 5GB의 GPU 메모리를 소비합니다.\n",
    "* 따라서, 최근 그래프 신경망을 더 큰 그래프로 확장하려는 노력이 있다.\n",
    "    * 이러한 접근법 중 하나는 Cluster-GCN(Chiang et al. (2019)으로 알려져 있으며, 이는 그래프를 미니 배치 방식으로 운영할 수 있는 하위 그래프(sub-graph)로 사전 파티셔닝(pre partitioning)하는 것을 기반으로 한다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='PubMed', transform=NormalizeFeatures())\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('==================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===============================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
    "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
      "Processing...\n",
      "Done!\n",
      "\n",
      "Dataset: PubMed():\n",
      "==================\n",
      "Number of graphs: 1\n",
      "Number of features: 500\n",
      "Number of classes: 3\n",
      "\n",
      "Data(edge_index=[2, 88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717])\n",
      "===============================================================================================================\n",
      "Number of nodes: 19717\n",
      "Number of edges: 88648\n",
      "Average node degree: 4.50\n",
      "Number of training nodes: 60\n",
      "Training node label rate: 0.003\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* node\n",
    "    * 19717 n\n",
    "    * 이 수의 노드는 GPU 메모리에 쉽게 맞아야 하지만 그럼에도 불구하고 PyTorch Geometric 내에서 GNN을 확장할 수 있는 방법을 보여주는 좋은 예입니다.\n",
    "* Cluster-GCN(Chiang et al. (2019)은 그래프 분할 알고리즘을 기반으로 그래프를 먼저 하위 그래프로 분할하여 작동합니다.\n",
    "\n",
    "따라서 GNN은 특정 하위 그래프 내에서만 교란하도록 제한되며, 이는 이웃의 폭발 문제를 생략합니다.\n",
    "\n",
    "그러나 그래프를 분할한 후 일부 링크가 제거되어 편향된 추정으로 인해 모델의 성능이 제한될 수 있습니다. \n",
    "\n",
    "또한 이 문제를 해결하기 위해 Cluster-GCN은 미니 배치 내에 클러스터 간 링크를 통합하고, 다음과 같은 확률적 분할 체계를 구축합니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from torch_geometric.data import ClusterData, ClusterLoader\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "cluster_data = ClusterData(data, num_parts=128)  # 1. Create subgraphs.\n",
    "train_loader = ClusterLoader(cluster_data, batch_size=32, shuffle=True)  # 2. Stochastic partioning scheme.\n",
    "\n",
    "print()\n",
    "total_num_nodes = 0\n",
    "for step, sub_data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of nodes in the current batch: {sub_data.num_nodes}')\n",
    "    print(sub_data)\n",
    "    print()\n",
    "    total_num_nodes += sub_data.num_nodes\n",
    "\n",
    "print(f'Iterated over {total_num_nodes} of {data.num_nodes} nodes!')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n",
      "\n",
      "Step 1:\n",
      "=======\n",
      "Number of nodes in the current batch: 4946\n",
      "Data(edge_index=[2, 15230], test_mask=[4946], train_mask=[4946], val_mask=[4946], x=[4946, 500], y=[4946])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of nodes in the current batch: 4916\n",
      "Data(edge_index=[2, 18420], test_mask=[4916], train_mask=[4916], val_mask=[4916], x=[4916, 500], y=[4916])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of nodes in the current batch: 4926\n",
      "Data(edge_index=[2, 16202], test_mask=[4926], train_mask=[4926], val_mask=[4926], x=[4926, 500], y=[4926])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of nodes in the current batch: 4929\n",
      "Data(edge_index=[2, 17260], test_mask=[4929], train_mask=[4929], val_mask=[4929], x=[4929, 500], y=[4929])\n",
      "\n",
      "Iterated over 19717 of 19717 nodes!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(500, 16)\n",
      "  (conv2): GCNConv(16, 3)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import sys\n",
    "class Printer():\n",
    "    \"\"\"Print things to stdout on one line dynamically\"\"\"\n",
    "    def __init__(self,num_period=10):\n",
    "        self.num_period = num_period\n",
    "        self.init_value = 0\n",
    "\n",
    "    def __call__(self,data) :\n",
    "        if self.init_value % self.num_period == 0 :\n",
    "            print('\\n'+data.__str__())\n",
    "            self.init_value = 1\n",
    "        else :\n",
    "            sys.stdout.write(\"\\r\\x1b[K\"+data.__str__())\n",
    "            sys.stdout.flush()\n",
    "            self.init_value += 1 \n",
    "\n",
    "printf = Printer(num_period=50)\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "\n",
    "      for sub_data in train_loader:  # Iterate over each mini-batch.\n",
    "          out = model(sub_data.x, sub_data.edge_index)  # Perform a single forward pass.\n",
    "          loss = criterion(out[sub_data.train_mask], sub_data.y[sub_data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "          loss.backward()  # Derive gradients.\n",
    "          optimizer.step()  # Update parameters based on gradients.\n",
    "          optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      \n",
    "      accs = []\n",
    "      for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "          correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
    "          accs.append(int(correct.sum()) / int(mask.sum()))  # Derive ratio of correct predictions.\n",
    "      return accs\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    printf(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch: 001, Train: 0.3333, Val Acc: 0.4160, Test Acc: 0.4070\n",
      "\u001b[KEpoch: 050, Train: 0.9833, Val Acc: 0.7980, Test Acc: 0.7810"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "test()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.9833333333333333, 0.798, 0.781]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [gnn examples](https://github.com/rusty1s/pytorch_geometric/tree/master/examples)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}